---
description: 
globs: 
alwaysApply: false
---
---

title: BackendÂ Blueprint â€” Conversational "MattBot"
kind: reference        # design & API docs (not hard rules)
scope: global          # visible to both Chat + Cmd+K (lightweight)
weight: 85             # loaded after design-system (higher number â†’ lower priority)
------------------------------------------------------------------------------------

# ğŸ› ï¸Â Backend Blueprint Â· Staticâ€‘site + Serverless Chatbot

> Goal: power a conversational assistant (â€œ**MattBot**â€) that answers visitorsâ€™ questions using **MCP** retrieval across multiple content sources, while the portfolio itself remains a static GHÂ Pages deployment.

## 1Â Â· Highâ€‘level Architecture

```
+------------+   HTTPS   +---------------------------+
|  Browser   | â”€â”€â”€â”€â”€â”€â”€â–¶ |  /api/chat (Edge Fn)      |
|  (GHÂ Pages)|          |  - Cloudflare Workers     |
+------------+          |  - Netlify Edge Functions |
                         +---------â”¬----------------+
                                   â”‚
                                   â–¼
                             Retrieval Layer (MCP)
                                   â”‚
                                   â–¼
                          OpenAI / Anthropic LLM
```

1. **Static frontâ€‘end** served via **GitHub Pages** (no servers).
2. Interactive chat widget posts to **`/api/chat`** â€” proxied to a serverless function on Cloudflare / Netlify (choose at deploy time).
3. The function invokes **MCP** (multiâ€‘context provider) to fetch snippets from:

   * MDX blog posts (`/content/**/*.mdx`)
   * Project READMEs (GitHub REST)
   * Designâ€‘system & docs (from this repo)
4. Retrieved chunks + user question â†’ fed to LLM via **RAG** prompt.
5. Stream response back to the browser (SSE).

---

## 2Â Â· MCP Retrieval Spec

| Source                       | Loader                                  | Filters            | Notes                        |
| ---------------------------- | --------------------------------------- | ------------------ | ---------------------------- |
| `content/**` MDX             | `mcp.loaders.local("content/**/*.mdx")` | strip frontâ€‘matter | Project & life writeâ€‘ups     |
| GitHub repos                 | `mcp.loaders.github(repo, branch)`      | `.md`, `README*`   | Use GitHub API token env var |
| Design docs (`.cursor/*.md`) | `mcp.loaders.local(".cursor/*.md")`     | exclude rules.md   | Provides site guidelines     |
| Public talks (optional)      | `mcp.loaders.url(RSS)`                  |                    | Transcript indexing later    |

```ts
const mcp = new MCP({
  embeddings: new OpenAIEmbeddings({ model: 'text-embedding-3-small' }),
  chunkSize: 800,
  chunkOverlap: 80,
});
```

---

## 3Â Â· API Endpoint Contract

`POST /api/chat`

```jsonc
{
  "sessionId": "uuid-v4",
  "message": "string (user prompt)",
  "history": [
    { "role": "user"|"assistant", "content": "..." }
  ]
}
```

**Response:** `200` Stream (text/eventâ€‘stream)

```text
event: token
data: { "content": "Hello, I'm MattBot..." }
```

### CORS

Allow `https://<username>.github.io` (production) & `http://localhost:3000` (dev).

---

## 4Â Â· Environment Variables (Edge Function)

| Key              | Purpose                           |
| ---------------- | --------------------------------- |
| `OPENAI_API_KEY` | LLM + embeddings                  |
| `GITHUB_TOKEN`   | Read private repos (optional)     |
| `MCP_CACHE_URL`  | Redis / KV URL for cached vectors |

> **Security:** **Never** expose keys in the Next.js client bundle. They live in Cloudflare `wrangler.toml` or Netlify UI.

---

## 5Â Â· Deployment Steps

1. **Build** static site â†’ `dist/` pushed to `gh-pages` branch.
   `pnpm run build && pnpm run export`
2. **Deploy** serverless function via CI:

   * CloudflareÂ Workers: `wrangler deploy`
   * Netlify: `netlify deploy --prod`
3. Save function URL to `.env.production` â†’ site widget points to it.

---

## 6Â Â· Roadmap

* [ ] ğŸŸ¢Â MVP: context = MDX blog & design docs, OpenAI GPTâ€‘4o.
* [ ] ğŸŸ¡Â Add live GitHub README retrieval.
* [ ] ğŸŸ¡Â Vector cache (Cloudflare D1 / Upstash Redis).
* [ ] ğŸŸ Â Auth (JWT) for private Q\&A.
* [ ] ğŸŸ£Â Voiceâ€‘toâ€‘text + audio replies (optional).
* [ ] ğŸ”µÂ Fineâ€‘tune persona on personal writings.

---

## 7Â Â· AI Guardâ€‘rails

* **No** direct code execution; embed links to source when suggesting code.
* Respond in firstâ€‘person â€œIâ€ as Matt, but clarify when unsure.
* If question outside personal scope, politely decline.

---

*Last updated: 2025â€‘06â€‘29*
