[
  {
    "title": "Machine Learning Research Project",
    "date": "2025-08-01T00:00:00.000Z",
    "description": "An in-depth look at my ongoing research in machine learning and artificial intelligence.",
    "tags": [
      "Research",
      "Machine Learning",
      "AI",
      "Python",
      "TensorFlow"
    ],
    "published": true,
    "timeToComplete": "6 months",
    "category": "Paper",
    "body": {
      "raw": "\n# Machine Learning Research Project\n\n## Project Overview\n\nThis research project focuses on developing novel machine learning approaches for improving natural language understanding in specialized domains. By combining recent advances in transformer architectures with domain-specific training techniques, we aim to create more efficient and accurate models for specific use cases.\n\n## Research Goals\n\n1. Develop new pre-training strategies for domain adaptation\n2. Reduce computational requirements while maintaining model performance\n3. Improve model interpretability for critical applications\n\n## Methodology\n\nOur approach combines several key techniques:\n\n```python\nimport tensorflow as tf\n\nclass DomainAdapter(tf.keras.Model):\n    def __init__(self, base_model, domain_layers):\n        super().__init__()\n        self.base = base_model\n        self.domain_specific = domain_layers\n        \n    def adapt(self, domain_data):\n        # Domain-specific adaptation logic\n        pass\n```\n\n## Preliminary Results\n\nInitial experiments have shown promising results:\n\n- 15% improvement in domain-specific tasks\n- 30% reduction in computational requirements\n- Better interpretability scores on standard benchmarks\n\n## Next Steps\n\nWe are currently working on:\n\n1. Expanding the dataset\n2. Implementing additional baseline comparisons\n3. Preparing for peer review\n\nStay tuned for more updates as the research progresses! ",
      "html": "<h1>Machine Learning Research Project</h1>\n<h2>Project Overview</h2>\n<p>This research project focuses on developing novel machine learning approaches for improving natural language understanding in specialized domains. By combining recent advances in transformer architectures with domain-specific training techniques, we aim to create more efficient and accurate models for specific use cases.</p>\n<h2>Research Goals</h2>\n<ol>\n<li>Develop new pre-training strategies for domain adaptation</li>\n<li>Reduce computational requirements while maintaining model performance</li>\n<li>Improve model interpretability for critical applications</li>\n</ol>\n<h2>Methodology</h2>\n<p>Our approach combines several key techniques:</p>\n<pre><code class=\"language-python\">import tensorflow as tf\n\nclass DomainAdapter(tf.keras.Model):\n    def __init__(self, base_model, domain_layers):\n        super().__init__()\n        self.base = base_model\n        self.domain_specific = domain_layers\n        \n    def adapt(self, domain_data):\n        # Domain-specific adaptation logic\n        pass\n</code></pre>\n<h2>Preliminary Results</h2>\n<p>Initial experiments have shown promising results:</p>\n<ul>\n<li>15% improvement in domain-specific tasks</li>\n<li>30% reduction in computational requirements</li>\n<li>Better interpretability scores on standard benchmarks</li>\n</ul>\n<h2>Next Steps</h2>\n<p>We are currently working on:</p>\n<ol>\n<li>Expanding the dataset</li>\n<li>Implementing additional baseline comparisons</li>\n<li>Preparing for peer review</li>\n</ol>\n<p>Stay tuned for more updates as the research progresses!</p>"
    },
    "_id": "blog/research-project.md",
    "_raw": {
      "sourceFilePath": "blog/research-project.md",
      "sourceFileName": "research-project.md",
      "sourceFileDir": "blog",
      "contentType": "markdown",
      "flattenedPath": "blog/research-project"
    },
    "type": "Blog",
    "slug": "research-project",
    "url": "/blog/research-project",
    "readingTime": 1
  },
  {
    "title": "What's the Vibe",
    "date": "2025-07-01T00:00:00.000Z",
    "description": "A deep dive into the next big thing: Vibe Coding.",
    "tags": [
      "AI",
      "Next.js",
      "TypeScript"
    ],
    "published": true,
    "image": "https://images.unsplash.com/photo-1677442136019-21780ecad995",
    "timeToComplete": "5 Hours",
    "category": "Project",
    "body": {
      "raw": "\n## Origin Story\n\n&nbsp;&nbsp;&nbsp;&nbsp;One day I was sitting in the lab when one of my friends stopped by and chatted. We were talking about how our respective research papers were going and he noticed I had chat pulled up(Chat is short for ChatGPT, it's just the slang now). He asked me what I was using chat for. Suprised, I just said I use chat for boiler plate code. After discussing how AI should be used in research, he introduced me to **Cursor**. Although he didn't use it personally, he had mention that a lot of his friends had been hopping on this wave to get code out quicker. \n\n&nbsp;&nbsp;&nbsp;&nbsp;A couple of months later, I was in a time crunch for a paper deadline. In my original implementation, I had messed up the dataloader implementation so now the way data was being passed in was wrong and now all of our metrics were wrong. One of my mentors had suggested to start using **Cursor** simply because it was really fast at coding and as long as you told the Large Language Model(LLMs) what to exactly change/fix your code would come out looking perfect. At first, it was a great tool, I was able to refactor a majority of my code and then start running metrics. Some other bugs had appeared but they were easy enough for me to fix on my own. However, about two weeks before the deadline, I was getting an random freeze. At first I thought it was some sort of resource lock due to the way we were distributing weights during initialization, but that didn't work. A couple of prompts turned into days and nights of trying to prompt the LLM to fix the bug. On occasion it would fix the bug but it would come back when I made another change. Eventually, I decided to roll up my sleves and go old fashion and search the internet for answers. A couple of google searches later, and voila, bug fixed(maybe I'll write about it later but essentially it was some bug with NVML not matching older systems on newer CUDA distributions). \n\n&nbsp;&nbsp;&nbsp;&nbsp; This got me thinking, what is the hype behind vibe coding? It was able to solve a lot of my problems, but when I really needed it, it wasn't able to solve the problem. Furthermore, it seems like it's impossible without going one day without seeing something about LLMs and Agentic AI changing our lives. But really will it? During my internship with Capital One, the team I was on was working on Call Summarization with RAG. It seems like more than ever, everyone is trying to capitalize on this AI boom. But really what can AI currently do in the hands of an CS student/junior SWE, someone who's supposed to be replaced by AI. So over the weekend, I decided to do some digging and find out really what this was all about.\n\n## Training Arc\n\n\n",
      "html": "<h2>Origin Story</h2>\n<p>    One day I was sitting in the lab when one of my friends stopped by and chatted. We were talking about how our respective research papers were going and he noticed I had chat pulled up(Chat is short for ChatGPT, it's just the slang now). He asked me what I was using chat for. Suprised, I just said I use chat for boiler plate code. After discussing how AI should be used in research, he introduced me to <strong>Cursor</strong>. Although he didn't use it personally, he had mention that a lot of his friends had been hopping on this wave to get code out quicker.</p>\n<p>    A couple of months later, I was in a time crunch for a paper deadline. In my original implementation, I had messed up the dataloader implementation so now the way data was being passed in was wrong and now all of our metrics were wrong. One of my mentors had suggested to start using <strong>Cursor</strong> simply because it was really fast at coding and as long as you told the Large Language Model(LLMs) what to exactly change/fix your code would come out looking perfect. At first, it was a great tool, I was able to refactor a majority of my code and then start running metrics. Some other bugs had appeared but they were easy enough for me to fix on my own. However, about two weeks before the deadline, I was getting an random freeze. At first I thought it was some sort of resource lock due to the way we were distributing weights during initialization, but that didn't work. A couple of prompts turned into days and nights of trying to prompt the LLM to fix the bug. On occasion it would fix the bug but it would come back when I made another change. Eventually, I decided to roll up my sleves and go old fashion and search the internet for answers. A couple of google searches later, and voila, bug fixed(maybe I'll write about it later but essentially it was some bug with NVML not matching older systems on newer CUDA distributions).</p>\n<p>     This got me thinking, what is the hype behind vibe coding? It was able to solve a lot of my problems, but when I really needed it, it wasn't able to solve the problem. Furthermore, it seems like it's impossible without going one day without seeing something about LLMs and Agentic AI changing our lives. But really will it? During my internship with Capital One, the team I was on was working on Call Summarization with RAG. It seems like more than ever, everyone is trying to capitalize on this AI boom. But really what can AI currently do in the hands of an CS student/junior SWE, someone who's supposed to be replaced by AI. So over the weekend, I decided to do some digging and find out really what this was all about.</p>\n<h2>Training Arc</h2>"
    },
    "_id": "blog/whats-the-vibe.md",
    "_raw": {
      "sourceFilePath": "blog/whats-the-vibe.md",
      "sourceFileName": "whats-the-vibe.md",
      "sourceFileDir": "blog",
      "contentType": "markdown",
      "flattenedPath": "blog/whats-the-vibe"
    },
    "type": "Blog",
    "slug": "whats-the-vibe",
    "url": "/blog/whats-the-vibe",
    "readingTime": 3
  }
]